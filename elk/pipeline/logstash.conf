input {
  # HTTP input to receive logs
  http {
    port => 5000  # The port where Logstash listens for HTTP requests
    codec => "json"  # This codec will automatically parse the JSON body into the event field
  }
}

filter {
  # If the event field exists, process the JSON body
  if [event] {
    # Rename and structure the event fields to match Elasticsearch mapping

    # Rename the 'original' field inside the 'event' object to 'event.original'
    mutate {
      rename => { "[event][original]" => "event.original" }
    }
    
    # Extract headers (assuming these come as part of the JSON object)
    mutate {
      add_field => { "headers" => "%{[http][headers]}" }
    }

    # Parse out message and non_json_content from event
    if [event][original] {
      mutate {
        add_field => { "message" => "%{[event][original]}" }
      }
      # Handle non-JSON content (in case the body has non-JSON data)
      mutate {
        add_field => { "non_json_content" => "true" }
      }
    }
  }

  # Optionally, handle invalid or non-JSON data with a fallback mechanism
  if "_jsonparsefailure" in [tags] {
    mutate {
      add_field => { "non_json_content" => "true" }
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }

  # Optionally, output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}
